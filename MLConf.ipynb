{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TweetGroups\n",
    "\n",
    "Twitter is an integral part of marketing and can’t be ignored.  Twitter interactions can not only be a good metric for tracking a marketing campaign’s performance, but it can also be the cause of product or brands success and failure.\n",
    "\n",
    "In recent years we have all seen examples of bad tweets that have ruined reputations and tarnished brands, so making sure that your company's tweets are throughly planned is essential.  The process of creating and maintaining an effective presence on Twitter is a complex one, but TweetGroups can help you get started:\n",
    "\n",
    "#### TweetGroups helps to answer two fundamental marketing questions:\n",
    "1.  What are our market segements (groups of customers)?\n",
    "2.  How do we engage these customers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TweetGroups uses the Twitter Search api to return a series of tweets that contain a specific query term (in this example we will use 'Go Pro').  Once the tweets are loaded, TweetGroups clusters all hashtags found in these Tweets via the text of the Tweets that contain them.  In order to do this we will need to follow a few pre-processing steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing with spaCy\n",
    "\n",
    "For text processing step we chose to use the Natural Language Processing library spaCy due to a few advantages over other libraries:\n",
    "\n",
    "1.  Performance: spaCy is written in Cython and contains a wide array of NLP functions that can be parallelized and execute quickly\n",
    "\n",
    "2.  Flexability: spaCy is an object-oriented approach, so the returned tokens have attributes that are particularly useful for social media data, including the ability to tokenize emojis, parts of speech tagging, and named entity recognition\n",
    "\n",
    "3.  Usability: spaCy code is clear, concise, well-documented and actively supported\n",
    "\n",
    "Let's show a quick performance comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import spacy\n",
    "import timeit\n",
    "import textacy\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading a pickled list of tweets\n",
    "import pandas as pd\n",
    "tweet_list = pd.read_pickle('/Users/nathanbackblaze/DS/Metis/projects/Final Projects/tweet_list_gopro.pkl')\n",
    "# this list contains about 14000 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textacy is tokenization package built on top of spacy that easily integrates the TFIDF and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 11.5 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit corpus = textacy.TextCorpus.from_texts('en',tweet_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCorpus(2510 docs; 232695 tokens)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Corpus' variable here is a Textacy object not only has the word tokens from our documents, but also has a suite of other attributes, including part of speech tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this in Sklearn we would need to do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_vectorize(docs):\n",
    "    vectorizer = CountVectorizer()\n",
    "    text_vec = vectorizer.fit_transform(docs)\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 252 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit count_vectorize(tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of course faster, but note that CounntVectorizer is only returning a simple bag of words, and does not have the array of features that spaCy provides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "having trouble finding the textblob equivalent, will look in further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference > http://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_textblob(docs):\n",
    "    tweetblob = []\n",
    "    for doc in docs:\n",
    "        tweetblob.extend(textblob.TextBlob(doc))\n",
    "    return tweetblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I can walkthrough the process using sklearn tools\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tokenized text matrix, we need to properly weight unimportant words.  Term Frequency Inverse Document Frequency will allow us automatically down-weight words that have less significance in determining the document's topic.\n",
    "\n",
    "This is especially cruicial in our case.  Because we are taking tweets that match a certain query term ('GoPro'), this means that many of these Tweets will contain the same words such as the query term itself.\n",
    "\n",
    "In the following step, we are also specifiying that we are tokenizing ngrams, or pairs and triplets of words.  This allows us to tokenize more specific semantic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_term_matrix, id2term = corpus.as_doc_term_matrix((doc.as_terms_list(words=True, ngrams=(2,3), named_entities=True)for doc in corpus),weighting='tfidf', normalize=True, smooth_idf=True, min_df=2, max_df=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Dimensionality Reduction Important?\n",
    "\n",
    "The text written on social media can be random, arbitrary, and have a wide variety of tokens (including words/phrases/emojis).  Without a way to reduce these high-dimensional token matricies, you can run in to performance issues and clustering may be difficult.  This is particularly import in our case, where we are using clustering algorthims that do not need a n_topics paraments (ie. Affinity Progpogation, DBSCAN, Mean Shift, etc.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (using Singular Value Decomposition)\n",
    "LSA is a simple and fast approach to dimensionality reduction.  It works to 'flatten' the dimensionality of the matrix while retaining as much variance as possible, and can work well with unbalanced topics.\n",
    "\n",
    "LSA does have a few shortcomings, however.  LSA can have trouble in cases where slight variance in words signify a different topic.  For instance, a colleague of mine (Emily) recently built an NLP model using Supreme Court data.  Most of these documents contained a large amount of similar legal jargon, and LSA was unable to effectively parse the latent topics.\n",
    "\n",
    "Textacy makes switching between dimensionality reduction methods easy, as you are simply passing in the chosen method as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 49.8 ms per loop\n",
      "1000 loops, best of 3: 1.41 ms per loop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2510, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = textacy.tm.TopicModel('lsa', n_topics=10)\n",
    "%timeit model.fit(doc_term_matrix)\n",
    "%timeit doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "doc_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned LSA (unlike LDA) values are not necessarily human readable, and don't represent a clear topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05693241, -0.02133441, -0.02307617, -0.02781793,  0.00016982,\n",
       "       -0.00223301, -0.01919703,  0.02144858,  0.00428661, -0.04453258])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic', 0, ':', u'gopro   $   11.99   camera   video   hero4   rt   gopro $   @gopro   hero')\n",
      "('topic', 1, ':', u'11.99   $   gopro $   $ 11.99   gopro $ 11.99   dog   13.99   harness   fetch   pet')\n",
      "('topic', 2, ':', u'hero4   camera   gopro hero4   silver   hero4 silver   gopro hero4 silver   edition   244.00   hero   gopro hero')\n",
      "('topic', 3, ':', u'scuba   diving   diva   padi   hawaii   sea   mar   underwater   scubadiving   gopro hero4')\n",
      "('topic', 4, ':', u'selfie   @gopro   @xgame @gopro   ion   deftfam   turnup   turnuptuesday   @xgame   texas   austin')\n",
      "('topic', 5, ':', u'drone   quad   fpv   13.99   dji   fly   airquad   atlantic\\u2026   naza   turnigy')\n",
      "('topic', 6, ':', u'13.99   $ 13.99   gopro $ 13.99   clamp   flex   adjustable   great   adjustable neck   mount with adjustable   neck')\n",
      "('topic', 7, ':', u'socialmedia   videoshoot   magmabags   denondj   editing   landrover   djlife   route   countryside   dj')\n",
      "('topic', 8, ':', u'gig   download   https://t.co/5zikmckbox   uk   vlog   montage   socialmedia   editing   route   denondj')\n",
      "('topic', 9, ':', u'drone   quad   socialmedia   denondj   editing   magmabags   videoshoot   route   countryside   dj')\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(id2term, top_n=10):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirchlet Allocation\n",
    "Pronounced 'deer_uh_shlay', this method assigns a probability that the document belongs to a given topic making the results more human readable.  LDA is iterative, so it begins by randomly assigning topical distributions and iterates through to optimize the assignments, so it will generally perform slower than LSA.\n",
    "\n",
    "LDA also has a few limitions.  When using LDA you assume a Dirchlet Prior, which stipulates that the original text documents are written in  the following manner.\n",
    "- You first decide a set number of words to that the document will have\n",
    "- You decide on a mixture of topics and draw words from those topics\n",
    "- The words you use are chosen from each topic 'corpus'\n",
    "\n",
    "As you can see, this assumption does not necessarily mesh well with how Tweets are actually written.  Tweets are approximately all of the same length (~ 140 characters), and you may only really be Tweeting about one topic, not choosing from a distribution.\n",
    "\n",
    "Furthermore, LDA is designed to find topics that are 'furthest' from each other, so LDA can have trouble identifying uneven topic distributions (which is often the case on Twitter).\n",
    "\n",
    "As a result, LDA is often better suited for understanding the distribution of a topics in the given corpus, rather than specifically classifying each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA requires and interger matrix, so TFIDF cannot be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_tf, id2term = corpus.as_doc_term_matrix((doc.as_terms_list(words=True, ngrams=(2,3), named_entities=True)for doc in corpus),weighting='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2510x37158 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 102213 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2510, 10)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = textacy.tm.TopicModel('lda', n_topics=10)\n",
    "model.fit(doc_term_matrix_tf)\n",
    "doc_topic_matrix_tf = model.transform(doc_term_matrix_tf)\n",
    "doc_topic_matrix_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10000046,  0.1000001 ,  0.10000051,  0.10000011,  0.10000051,\n",
       "        0.10000021,  0.10000032,  0.10000075,  3.97297869,  0.10000169])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix_tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic', 0, ':', u'gopro   13.99   morning   $ 13.99   $   gopro $ 13.99   gopro $   happy   travel   rt')\n",
      "('topic', 1, ':', u'gopro   3dprint   give   \\U0001f440   blackdress   link   cape   littleblackridinghood   pullup   vintage')\n",
      "('topic', 2, ':', u'gopro   2016   vscocam   vsco   brasil   camera   care   panda   studio   landrover')\n",
      "('topic', 3, ':', u'gopro   europe   fly   agario   turkey   fake   slitherio   turkiye   afk   oyun')\n",
      "('topic', 4, ':', u'gopro   @gopro   drone   fun   rt   quad   video   parkour   freerunning   @goprouk')\n",
      "('topic', 5, ':', u'gopro   summer   travel   thailand   incredible   video   photography   blogger   leh   ladakh')\n",
      "('topic', 6, ':', u'gopro   travel   gopro\\u306e\\u3042\\u308b\\u751f\\u6d3b   @gopro   oahu   nassau   dallas   goprohero4   hawaii   calpe')\n",
      "('topic', 7, ':', u'gopro   venice   munich   snorkel   best   today   \\u6c96\\u7e04   edit   dji   best sale')\n",
      "('topic', 8, ':', u'gopro   rt   video   goprohero4   hero4   @gopro   camera   gopro\\u2026   hero   summer')\n",
      "('topic', 9, ':', u'gopro   selfie   \\u5bcc\\u58eb\\u5c71   @gopro   download   2016   music   gig   vlog   lens')\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(id2term, top_n=10):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization (NMF)\n",
    "True to it's name, this newer method requires that the passed in matrix be non-negative.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
